# src/stage_2/build_feature_tables.py

from pathlib import Path
import pandas as pd

# Import feature extraction and npz data loading functions
from .extract_features import (
    load_npz_data,
    compute_time_domain_features,
    compute_freq_domain_features,
)

# Build a row as a dictionary with features and associated values
# Will include the label and a couple of metadata fields as
def build_row(*feature_dicts: dict[str, object]):
    row = {}
    for feature in feature_dicts: # Iterate through all the features and add them to the row dict
        row.update(feature)
    return row # Return a dictionary which resembles a row

# Process a single npz recording, extract features, and append in a list of rows
def process_npz(npz_path: Path, split: str, rows: list[dict[str, object]]):

    data = load_npz_data(npz_path) # Load npz data through helper function

    X = data["X"] # 3D NumPy array with EEG data (pre-processed features)
    y = data["y"].astype(int) # 1D NumPy array with numeric sleep stage labels (pre-processed labels)
    starts = data["starts"] # 1D NumPy array with start times of each epoch in seconds
    channels = data["channels"] # 1D NumPy array with both the channels
    sfreq = float(data["sfreq"]) # 1D NumPy array storing the sampling rate
    subject_id = str(data["subject_id"]) # 1D NumPy array storing the subject_id
    night = int(data["night"]) # 1D NumPy array storing the corresponding night of a subject_id

    # Initialize variables to store index of channels in the 2D NumPy array 
    fpz_index = None
    pz_index = None

    # Iterate through channels array to assign channel index rather than hardcoding
    for index, ch in enumerate(channels):
        name = ch.lower().replace(" ", "") # Remove any extra white spaces and turn to lowercase
        if ("fpz" in name and "cz" in name): # If "fpz" and "cz" exist, save index to variable
            fpz_index = index
        elif ("pz" in name and "oz" in name): # If "pz" and "oz" exist, save index to variable
            pz_index = index

    # If either of the channels couldn't be found, print warning
    if fpz_index is None or pz_index is None:
        print(f"Skipping {npz_path.name} because missing Fpz-Cz or Pz-Oz channels")
        return None
    
    number_of_epochs = int(X.shape[0]) # Get number of epochs from X

    # Iterate through all the epochs
    for epoch_id in range(number_of_epochs):

        # Extract samples (EEG data) for the particular epoch from each channel
        epoch_fpz = X[epoch_id, fpz_index, :] 
        epoch_pz  = X[epoch_id, pz_index, :]

        # Compute time domain features for both channels
        tdf_fpz = compute_time_domain_features(epoch_fpz, "fpz")
        tdf_pz  = compute_time_domain_features(epoch_pz,  "pz")

        # Compute frequency domain features for both channels
        fdf_fpz = compute_freq_domain_features(epoch_fpz, sfreq, "fpz")
        fdf_pz  = compute_freq_domain_features(epoch_pz,  sfreq, "pz")

        # Metadata to include in row for this epoch
        meta = {
            "subject_id": subject_id, # subject_id of the person
            "night": night, # night corresponding to a subject_id's recording
            "epoch_id": epoch_id,  # Index of epoch which will act as ID
            "split": split, # Train, cv, or test
            "epoch_start_point": float(starts[epoch_id]), # Start time of the epoch in seconds
            "sfreq": sfreq,  # 1D NumPy array storing the sampling rate
            "sleep_stage_int_value": int(y[epoch_id]), # Numeric sleep stage label (0â€“4)
        }

        # Build and append row (meta + features)
        row = build_row(meta, tdf_fpz, tdf_pz, fdf_fpz, fdf_pz) # Merge all the dictionaries in one big dictionary which acts as row
        rows.append(row) # List of rows which will be converted to csv

# Builds the feature table (csv) for a particular split (train, cv, or test)
def build_csv_for_split(
    split: str,
    preprocessed_root: Path = Path("data/preprocessed/npz"),
    output_root: Path = Path("data/processed/features"),
):
    npz_directory = preprocessed_root / split # Path to directory where npz files are stored (split wise)
    output_root.mkdir(parents = True, exist_ok = True) # Create directory to store feature tables (csv) split wise

    out_csv = output_root / f"{split}_features.csv" # Path for feature csv file

    rows: list[dict[str, object]] = [] # Initialize empty list to store feature dictionaries for each epoch

    npz_files = sorted(npz_directory.glob("*.npz")) # Sort npz directory aphabetically

    # If no files found print warning
    if not npz_files:
        print(f"No NPZ files found for '{split}' at {npz_directory}")
        return None
    
    # Iterate through all npz files
    for npz_path in npz_files:
        # If any of the npz files can't be loaded then print error
        try:
            process_npz(npz_path, split, rows)
        except Exception as e:
            print(f"Failed processing {npz_path.name}: {e}")

    # If the rows list has features (successful processing of npz files)
    if rows:
        df = pd.DataFrame(rows) # Convert list of dictionaries to DataFrame
        total_nas = int(df.isna().sum().sum()) # Count total number of missing values (null, NaNa, None, etc)
        out_csv.parent.mkdir(parents = True, exist_ok = True) # Build full file path for csv outpt
        df.to_csv(out_csv, index = False) # Convert the DataFrame to csv

        # Print summary
        print(f"\nFEATURE TABLE BUILT FOR {split}\n")
        print(f"Rows (epochs): {len(df)}")
        print(f"Columns (meta + features): {df.shape[1]}")
        print(f"Null values: {total_nas}")
        print(f"Output CSV  {out_csv}")

    else:
        print(f"No rows for {split} check {npz_directory}") # If nothing was processed successfully

